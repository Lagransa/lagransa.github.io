{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b0bd1d5-f629-467c-949b-73dc938b8861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import dataset as Dataset\n",
    "from torch.utils.data import dataloader as DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8064b294-c408-4130-9bb8-04c4466607b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([  5,  17,  19,  26,  28,  29,  31,  32,  36,  42,\n",
       "       ...\n",
       "       832, 837, 839, 846, 849, 859, 863, 868, 878, 888],\n",
       "      dtype='int64', length=177)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = %pwd\n",
    "data = pd.read_csv(f'{test}/kaggle1_data/train.csv')\n",
    "# print(data.duplicated().sum()) #=0\n",
    "newdf = data.interpolate()\n",
    "diff = newdf['Age'].compare(data['Age'])\n",
    "diff.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06adbb88-c689-46ed-b019-8498e9f1a304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.682"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###计算熵增益 H(Survived|Pclass) = -p1log2p1-p2log2p2\n",
    "#####################耻辱柱：条件不要加df[]，会变成df&df报错！！！\n",
    "# survive_cls1 = newdf[(newdf['Pclass']==1) & (newdf['Survived']==1)]\n",
    "# cls1 = newdf[newdf['Pclass']==1]\n",
    "\n",
    "lenth = len(newdf)\n",
    "pct = [[] for i in range(2)] #0.63, 0.47, 0.24   H(S|1)  H(S|2)  H(S|3)\n",
    "for i in range(1, 4):\n",
    "    all_mem = len(newdf[newdf['Pclass']==i])\n",
    "    survive = len(newdf[(newdf['Pclass']==i) & (newdf['Survived']==1)])\n",
    "    sur_pct = round(survive/all_mem, 2) #活下来的占该cls的比例\n",
    "    mem_pct = round(all_mem/lenth, 2) #该class占总的比例\n",
    "    pct[0].append(sur_pct)\n",
    "    pct[1].append(mem_pct)\n",
    "\n",
    "HSx_1 = round(-np.log2(pct[0][0]) * pct[0][0] - np.log2(1 - pct[0][0]) * (1 - pct[0][0]), 2) #0.95\n",
    "HSx_2 = round(-np.log2(pct[0][1]) * pct[0][1] - np.log2(1 - pct[0][1]) * (1 - pct[0][1]), 2) #1\n",
    "HSx_3 = round(-np.log2(pct[0][2]) * pct[0][2] - np.log2(1 - pct[0][2]) * (1 - pct[0][2]), 2) #0.8\n",
    "HSx = pct[1][0] * HSx_1 + pct[1][1] * HSx_2 + pct[1][2] * HSx_3 #0.878\n",
    "HS = round(-np.log2(0.3)*0.3 * 3, 2) #1.56\n",
    "HS_gain = HS - HSx #熵增益\n",
    "HS_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51609de3-132f-41a8-ba6f-85a9d750b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.loc[newdf['Sex']=='male', 'Sex'] = 0\n",
    "newdf.loc[newdf['Sex']=='female', 'Sex'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "736f160e-ae8e-4805-a6f3-59c4e10a1dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_HS_gain(col, df):\n",
    "    assert type(col) == str\n",
    "    typ = list(df.groupby(col).groups.keys())\n",
    "    num = range(len(typ))\n",
    "    HS_x = []\n",
    "    sur_pc = []\n",
    "    for j in num:\n",
    "        ind = typ[j]\n",
    "        condition1 = df['Survived'] == 1\n",
    "        condition2 = df[col] == ind\n",
    "        intersection = df[(condition1) & (condition2)]\n",
    "        sur_pct = round(len(intersection) / len(df[condition2]), 2) #注意condition2是索引不是df，默认长度等于df的\n",
    "        all_pct = round(len(df[condition2]) / len(df), 2) #这里也是\n",
    "        hs_xj = -np.log2(sur_pct) * sur_pct - np.log2(1 - sur_pct) * (1 - sur_pct)\n",
    "        hs_part = hs_xj * all_pct\n",
    "        HS_x.append(hs_part)\n",
    "        sur_pc.append([sur_pct, all_pct, hs_xj, ind, condition2, len(df)])\n",
    "    uniform = round(1 / len(typ), 2)\n",
    "    HS_h0 = -np.log2(uniform) #后面*uniform*num等于乘以1了就不写了\n",
    "    HS_observe = sum(HS_x)\n",
    "    return HS_h0 - HS_observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d5c9083-ac2b-433b-8c6b-c70a7f6302a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7245742809106628"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_HS_gain('Pclass', newdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b10a7dd-8820-4389-a43c-8834eaef356c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_idx = newdf.dtypes[newdf.dtypes != 'object'].index\n",
    "label_set = torch.tensor(newdf[numeric_idx]['Survived'], dtype=torch.float32)\n",
    "newdf_d = newdf[numeric_idx].drop(['Parch', 'PassengerId', 'Survived'], axis=1)\n",
    "newdf_d = newdf_d.apply(lambda x: (x-x.mean())/x.std())\n",
    "# newdf_d.plot(kind='scatter', x='Survived', y='Fare')\n",
    "# plt.show()\n",
    "newdf_d1 = pd.get_dummies(newdf_d, columns=['Pclass', 'SibSp']).astype(float)\n",
    "# newdf_d1\n",
    "dataset = newdf_d1.values\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e39787a4-ee5d-4aac-a1c9-3a6008fa36a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_test1(Dataset.Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.Tensor(self.data[index]), torch.Tensor(self.label[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bfdf1a2-551a-45a7-aada-01dfb1485947",
   "metadata": {},
   "outputs": [],
   "source": [
    "pct = 0.6\n",
    "num_train = round(len(dataset) * 0.6)\n",
    "train_set, train_label = dataset[:num_train], label_set[:num_train]\n",
    "test_set, test_label = dataset[num_train:], label_set[num_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18ae1f2e-9483-4df6-b060-4188ece00ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Data_test1(train_set, train_label)\n",
    "test_data = Data_test1(test_set, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63982d23-9658-42b1-b6a6-9534326cd9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = DataLoader.DataLoader(dataset=train_data, batch_size=100, shuffle=True)\n",
    "test_iter = DataLoader.DataLoader(dataset=test_data, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6957ac5-b12f-4fec-9c43-cf89b4f13750",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "class test_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential()\n",
    "        self.net.add_module('linear1', nn.Linear(12, 24))\n",
    "        self.net.add_module('relu1', nn.ReLU())\n",
    "        self.net.add_module('linear2', nn.Linear(24, 10))\n",
    "        self.net.add_module('relu2', nn.ReLU())\n",
    "        self.net.add_module('dense', nn.Linear(10, 2))\n",
    "    def forward(self, X):\n",
    "        for net in self.net:\n",
    "            X = net(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85b90d49-4956-4702-9358-7934338dabf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, num_epochs, train_iter, loss, lr):\n",
    "    if isinstance(net, nn.Module):\n",
    "        net.train()\n",
    "    train_ls = []\n",
    "    metrics = Accumulator(3)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        for X, y in train_iter:\n",
    "            y = y.type(torch.long)\n",
    "            # y = y.reshape((-1, 1))\n",
    "            # print(X.shape, y.shape)\n",
    "            optimizer.zero_grad()\n",
    "            l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            metrics.add(float(l * y.numel()), accuracy(net(X), y)[0], y.numel())\n",
    "        if epoch % 50 == 0:\n",
    "            train_ls.append([metrics[0] / metrics[2] , metrics[1] / metrics[2], accuracy(net(X), y)[1]]) \n",
    "    return train_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c58dbaee-92b4-4bbc-b640-a6ce57129714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    ##自制auc曲线看一下 TPP和FPP不是除以所有！是除以True P!   True F!\n",
    "    auc = []\n",
    "    counttp = 0\n",
    "    countfp = 0\n",
    "    for i in range(y.numel()):\n",
    "        if y[i] - y_hat.type(y.dtype)[i] == -1:\n",
    "            countfp += 1\n",
    "        if (y[i] - y_hat.type(y.dtype)[i] == 0) and (y[i] == 1):\n",
    "            counttp += 1\n",
    "    auc.append([counttp / y[y==1].sum(), countfp / y[y==0].sum()])\n",
    "    return float(cmp.type(y_hat.dtype).sum()), auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbf5acd7-5ab7-4176-a4eb-4230ab158564",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5ac4b01-01f7-484b-9479-31a1b498401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(net, data_iter):\n",
    "    if isinstance(net, nn.Module):\n",
    "        net.eval()\n",
    "    metric = Accumulator(2)\n",
    "    for X, y in data_iter:\n",
    "        metric.add(accuracy(net(X), y)[0], y.numel())\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba6b9a04-992b-4e2e-81bf-ee85dfcf3614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (data, label) in enumerate(train_iter):\n",
    "#     if i==2:\n",
    "#         break\n",
    "#     print(net(data), label.reshape((-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "512f4c9a-9949-4278-a525-ab996a2e31d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.6545580641131534, 0.6598130841121496, [[tensor(0.5556), tensor(inf)]]],\n",
       " [0.5701261342827907, 0.7087044163459777, [[tensor(0.3125), tensor(inf)]]],\n",
       " [0.547565238813561, 0.7200703247894883, [[tensor(0.7333), tensor(inf)]]],\n",
       " [0.5339787217541748, 0.7264467413504982, [[tensor(0.5625), tensor(inf)]]],\n",
       " [0.5227701298522168, 0.7326451852885107, [[tensor(0.7895), tensor(inf)]]],\n",
       " [0.5152678839629594, 0.7365751945489072, [[tensor(0.6250), tensor(inf)]]],\n",
       " [0.5078608767856825, 0.7404042599434906, [[tensor(0.6364), tensor(inf)]]],\n",
       " [0.5033900100011638, 0.7430518944537636, [[tensor(0.4667), tensor(inf)]]],\n",
       " [0.49878131653880414, 0.7449926585405645, [[tensor(0.7273), tensor(inf)]]],\n",
       " [0.49594783129218667, 0.7469051122116999, [[tensor(0.6364), tensor(inf)]]],\n",
       " [0.493261951956967, 0.7484768780196616, [[tensor(0.6667), tensor(inf)]]],\n",
       " [0.4905405004148614, 0.7496616177892362, [[tensor(0.6000), tensor(inf)]]],\n",
       " [0.48867020688633905, 0.7510317694807719, [[tensor(0.8000), tensor(inf)]]],\n",
       " [0.48640305315532456, 0.7527743083968589, [[tensor(0.5294), tensor(inf)]]],\n",
       " [0.484769617430311, 0.7537669817483702, [[tensor(0.8182), tensor(inf)]]],\n",
       " [0.4828767132984289, 0.7547817862787312, [[tensor(0.6316), tensor(inf)]]],\n",
       " [0.4807568175603505, 0.7558915841179834, [[tensor(0.6667), tensor(inf)]]],\n",
       " [0.4792019573663994, 0.7564953820134641, [[tensor(0.7857), tensor(inf)]]],\n",
       " [0.4777701347193037, 0.7569595568786499, [[tensor(0.6923), tensor(inf)]]],\n",
       " [0.47792438998289316, 0.7571921342020697, [[tensor(0.9000), tensor(inf)]]]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = test_net()\n",
    "# test_X = torch.ones((2, 12))\n",
    "# net.parameters()\n",
    "train(net, 1000, train_iter, loss, 0.03)\n",
    "# net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "725ceb24-2848-4c5c-92af-37924cb2d8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 0, 0]), tensor([0, 1, 0]), torch.int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.empty(3, dtype=torch.long).random_(3)\n",
    "b = torch.randn((3, 3)).argmax(axis=1)\n",
    "a, b, b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "554e87c9-a7d2-4f72-b689-328d76b8fdd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1, 0], [0, 1], [1, 0]])\n",
    "b = torch.tensor([1, 0, 1])\n",
    "a[a.argmax(axis=1)==0].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
